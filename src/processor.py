"""ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì²˜ë¦¬: ì¤‘ë³µ ì œê±°, ë‚ ì§œ í•„í„°ë§, ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜."""

from __future__ import annotations

import json
import logging
import os
from datetime import datetime, timedelta

from src.config import DAYS_LOOKBACK, HISTORY_FILE
from src.crawlers.base import Article

logger = logging.getLogger(__name__)

# ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í‚¤ì›Œë“œ ë§¤í•‘
CATEGORY_KEYWORDS: dict[str, list[str]] = {
    "ğŸ“‹ ì§€ì›ì‚¬ì—… ê³µê³ ": [
        "ê³µê³ ", "ëª¨ì§‘", "ì‹ ì²­", "ì ‘ìˆ˜", "íŒ¨í‚¤ì§€", "ë°”ìš°ì²˜", "ì‚¬ì—…ê³µê³ ",
        "ì§€ì›ì‚¬ì—…", "ì°¸ì—¬ê¸°ì—…", "ì„ ì •",
    ],
    "ğŸ’° íˆ¬ì/ìê¸ˆ ì§€ì›": [
        "íˆ¬ì", "í€ë“œ", "ìê¸ˆ", "ëŒ€ì¶œ", "ë³´ì¦", "ìœµì", "ì¶œì",
    ],
    "ğŸ“ êµìœ¡/ë©˜í† ë§": [
        "êµìœ¡", "ë©˜í† ë§", "ì»¨ì„¤íŒ…", "ì•„ì¹´ë°ë¯¸", "íŠ¹ê°•", "ì›Œí¬ìˆ", "ì„¸ë¯¸ë‚˜",
    ],
    "ğŸ“° ì •ì±… ë‰´ìŠ¤": [],  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ (ë§¤ì¹­ ì•ˆ ë˜ë©´ ì—¬ê¸°ë¡œ)
}


def classify(article: Article) -> str:
    """ê¸°ì‚¬ ì œëª© ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜."""
    title = article.title
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(kw in title for kw in keywords):
            return category
    return "ğŸ“° ì •ì±… ë‰´ìŠ¤"


def load_history() -> set[str]:
    """ì „ì†¡ ì´ë ¥(URL ëª©ë¡)ì„ ë¡œë“œ."""
    if not os.path.exists(HISTORY_FILE):
        return set()
    try:
        with open(HISTORY_FILE, encoding="utf-8") as f:
            return set(json.load(f))
    except (json.JSONDecodeError, OSError):
        return set()


def save_history(urls: set[str]) -> None:
    """ì „ì†¡ ì´ë ¥ ì €ì¥. ìµœê·¼ 500ê±´ë§Œ ìœ ì§€."""
    url_list = sorted(urls)[-500:]
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(url_list, f, ensure_ascii=False, indent=2)


def process(articles: list[Article]) -> dict[str, list[Article]]:
    """ê¸°ì‚¬ ëª©ë¡ì„ ì²˜ë¦¬í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ëœ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜.

    1. ë‚ ì§œ í•„í„°ë§ (ìµœê·¼ Nì¼)
    2. ì¤‘ë³µ ì œê±° (ì´ì „ ì „ì†¡ ì´ë ¥ ê¸°ë°˜)
    3. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    4. ì „ì†¡ ì´ë ¥ ì—…ë°ì´íŠ¸
    """
    cutoff = datetime.now() - timedelta(days=DAYS_LOOKBACK)
    history = load_history()
    result: dict[str, list[Article]] = {}
    new_urls: set[str] = set()

    for article in articles:
        # ë‚ ì§œ í•„í„°ë§
        dt = article.date_obj
        if dt and dt < cutoff:
            continue

        # ì¤‘ë³µ ì œê±°
        if article.url in history:
            continue

        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        article.category = classify(article)
        result.setdefault(article.category, []).append(article)
        new_urls.add(article.url)

    # ê° ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ ë‚ ì§œ ì—­ìˆœ ì •ë ¬
    for cat in result:
        result[cat].sort(key=lambda a: a.date, reverse=True)

    # ì „ì†¡ ì´ë ¥ ì—…ë°ì´íŠ¸
    if new_urls:
        save_history(history | new_urls)

    total = sum(len(v) for v in result.values())
    logger.info("ì²˜ë¦¬ ì™„ë£Œ: ì´ %dê±´ (ì‹ ê·œ), %dê±´ í•„í„°ë§ë¨", total, len(articles) - total)
    return result
