"""ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì²˜ë¦¬: ì¤‘ë³µ ì œê±°, ë‚ ì§œ í•„í„°ë§, í–‰ë™ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜."""

from __future__ import annotations

import json
import logging
import os
from datetime import datetime, timedelta

from src.config import DAYS_LOOKBACK, HISTORY_FILE
from src.crawlers.base import Article

logger = logging.getLogger(__name__)

# ë‰´ìŠ¤ ì†ŒìŠ¤ (ë§ˆê°ì¼ ì—†ëŠ” ì •ë³´ì„± ì½˜í…ì¸ )
NEWS_SOURCES = {"ë„¤ì´ë²„ë‰´ìŠ¤", "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€"}

# ì¹´í…Œê³ ë¦¬ ì •ì˜
CAT_URGENT = "ğŸ”¥ ë§ˆê° ì„ë°•"
CAT_NEW = "ğŸ“‹ ì‹ ê·œ ê³µê³ "
CAT_NEWS = "ğŸ“° ì •ì±… ë™í–¥"

# ë©”ì¸ ë©”ì‹œì§€ í‘œì‹œ ì œí•œ
LIMITS = {
    CAT_URGENT: None,  # ì „ì²´ í‘œì‹œ
    CAT_NEW: 10,
    CAT_NEWS: 5,
}

URGENT_DAYS = 7  # D-7 ì´ë‚´ë©´ ë§ˆê° ì„ë°•


def classify(article: Article) -> str:
    """ê¸°ì‚¬ë¥¼ í–‰ë™ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜."""
    # ë‰´ìŠ¤ ì†ŒìŠ¤ â†’ ì •ì±… ë™í–¥
    if article.source in NEWS_SOURCES:
        return CAT_NEWS

    # ë§ˆê°ì¼ì´ ìˆê³  D-7 ì´ë‚´ â†’ ë§ˆê° ì„ë°•
    d_day = article.d_day
    if d_day is not None and 0 <= d_day <= URGENT_DAYS:
        return CAT_URGENT

    # ë‚˜ë¨¸ì§€ ê³µê³  â†’ ì‹ ê·œ ê³µê³ 
    return CAT_NEW


def load_history() -> set[str]:
    """ì „ì†¡ ì´ë ¥(URL ëª©ë¡)ì„ ë¡œë“œ."""
    if not os.path.exists(HISTORY_FILE):
        return set()
    try:
        with open(HISTORY_FILE, encoding="utf-8") as f:
            return set(json.load(f))
    except (json.JSONDecodeError, OSError):
        return set()


def save_history(urls: set[str]) -> None:
    """ì „ì†¡ ì´ë ¥ ì €ì¥. ìµœê·¼ 500ê±´ë§Œ ìœ ì§€."""
    url_list = sorted(urls)[-500:]
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(url_list, f, ensure_ascii=False, indent=2)


def process(articles: list[Article]) -> dict[str, list[Article]]:
    """ê¸°ì‚¬ ëª©ë¡ì„ ì²˜ë¦¬í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ëœ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜."""
    cutoff = datetime.now() - timedelta(days=DAYS_LOOKBACK)
    history = load_history()
    result: dict[str, list[Article]] = {}
    new_urls: set[str] = set()

    for article in articles:
        # ë‚ ì§œ í•„í„°ë§ (ë‰´ìŠ¤ë§Œ â€” ê³µê³ ëŠ” ë§ˆê° ì „ì´ë©´ í‘œì‹œ)
        if article.source in NEWS_SOURCES:
            dt = article.date_obj
            if dt and dt < cutoff:
                continue

        # ë§ˆê°ëœ ê³µê³  ì œì™¸
        d_day = article.d_day
        if d_day is not None and d_day < 0:
            continue

        # ì¤‘ë³µ ì œê±°
        if article.url in history:
            continue

        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        article.category = classify(article)
        result.setdefault(article.category, []).append(article)
        new_urls.add(article.url)

    # ë§ˆê° ì„ë°•: D-day ì˜¤ë¦„ì°¨ìˆœ (ê¸‰í•œ ê²ƒ ë¨¼ì €)
    if CAT_URGENT in result:
        result[CAT_URGENT].sort(key=lambda a: a.d_day if a.d_day is not None else 999)

    # ì‹ ê·œ ê³µê³ : ë“±ë¡ì¼ ì—­ìˆœ
    if CAT_NEW in result:
        result[CAT_NEW].sort(key=lambda a: a.date, reverse=True)

    # ì •ì±… ë™í–¥: ë°œí–‰ì¼ ì—­ìˆœ
    if CAT_NEWS in result:
        result[CAT_NEWS].sort(key=lambda a: a.date, reverse=True)

    # ì „ì†¡ ì´ë ¥ ì—…ë°ì´íŠ¸
    if new_urls:
        save_history(history | new_urls)

    total = sum(len(v) for v in result.values())
    logger.info("ì²˜ë¦¬ ì™„ë£Œ: ì´ %dê±´ (ì‹ ê·œ), %dê±´ í•„í„°ë§ë¨", total, len(articles) - total)
    return result
